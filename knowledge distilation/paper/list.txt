1.Knowledge distillation_ A good teacher is patient and consistent
2.Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification
3.(reviewkd_cvpr2021)Distilling Knowledge via Knowledge Review
